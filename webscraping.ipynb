{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import html\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 'YOJNMIRO64OhCZjBIWEO'\n",
    "client_secret = 'ZHs9hqW8b9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['네이버', '카카오', '라인', '쿠팡', '배달의민족', '당근마켓', '토스', '직방', '야놀자', '삼성전자', 'SK하이닉스', 'DB하이텍']\n",
    "\n",
    "driver = webdriver.Chrome('/Users/leehyounshu/Documents/2024NLP/project/chromedriver-mac-x64/chromedriver')\n",
    "\n",
    "for keyword in keywords: \n",
    "    base_url = 'https://openapi.naver.com/v1/search/news.json'\n",
    "    encQuery = urllib.parse.quote(keyword) # 검색어\n",
    "    n_display = 100 # 한 번에 표시할 검색 결과 개수\n",
    "    sort = 'sim' # 검색 결과 정렬 방법: 정확도순으로 내림차순 정렬\n",
    "    \n",
    "    result_list = [] # 검색 결과 저장을 위한 빈 리스트 생성\n",
    "    for start in range(1, 1000, 100): \n",
    "        # API 요청 URL 생성\n",
    "        url = f'{base_url}?query={encQuery}&display={n_display}&start={start}&sort={sort}'\n",
    "\n",
    "        my_request = urllib.request.Request(url) \n",
    "        my_request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        my_request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "\n",
    "        response = urllib.request.urlopen(my_request) # API 요청 보내고 응답 받기\n",
    "        rescode = response.getcode() # 응답 코드 확인\n",
    "        \n",
    "        if(rescode==200): # 응답 코드가 성공 인지 확인\n",
    "            response_body = response.read() # 응답 본문 읽기\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "            \n",
    "        search_results = response_body.decode('utf-8') # 응답 본문을 문자열로 디코딩\n",
    "        search_results = eval(search_results) # 문자열을 딕셔너리 형태로 변환\n",
    "        \n",
    "        result_list.append(search_results) # 검색 결과를 리스트에 추가\n",
    "    \n",
    "    \n",
    "    news_links = [] # 뉴스 링크를 저장할 리스트 생성\n",
    "    for result in result_list:\n",
    "        for item in (result['items']):\n",
    "            # 링크 추출 후 HTML 엔티티를 디코딩하고 역슬래시를 제거\n",
    "            link = html.unescape(item['link']).replace('\\\\', '')\n",
    "            if 'n.news.naver.com' in link: # 네이버 뉴스 링크인지 확인\n",
    "                news_links.append(link) # 네이버 뉴스 링크를 리스트에 추가\n",
    "        \n",
    "    all_results = dict() # 모든 결과를 저장할 딕셔너리 생성\n",
    "\n",
    "    for idx in tqdm(range(len(news_links))):\n",
    "        link = news_links[idx] # 현재 인덱스의 링크 추출\n",
    "        all_results[idx] = dict() # 인덱스를 키로 사용하여 딕셔너리 초기화\n",
    "        driver.get(link) # 웹 드라이버로 해당 링크 열기\n",
    "        time.sleep(1) # 페이지 로딩을 위해 1초 대기\n",
    "        \n",
    "        try: # 제목 요소 찾기\n",
    "            title = driver.find_element(By.CLASS_NAME, 'media_end_head_headline')\n",
    "        except NoSuchElementException: # 요소를 찾을 수 없으면 다음 링크로 넘어감\n",
    "            continue\n",
    "        \n",
    "        title = title.text # 제목 텍스트 추출\n",
    "        \n",
    "        body = driver.find_element(By.ID, 'newsct_article') # 본문 요소 찾기\n",
    "        body = body.text.replace('\\n', '') # 본문 텍스트 추출 후 줄바꿈 문자 제거\n",
    "\n",
    "        # 결과 딕셔너리에 URL, 제목, 본문 추가\n",
    "        all_results[idx]['url'] = link\n",
    "        all_results[idx]['title'] = title\n",
    "        all_results[idx]['content'] = body\n",
    "\n",
    "    df = pd.DataFrame(all_results).T # 딕셔너리를 데이터프레임으로 변환 후 전치\n",
    "    \n",
    "    # 데이터프레임을 CSV 파일로 저장\n",
    "    df.to_csv('csv/{}_뉴스.csv'.format(keyword), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['네이버', '카카오', '라인', '쿠팡', '배달의민족', '당근마켓', '토스', '직방', '야놀자', '삼성전자', 'SK하이닉스', 'DB하이텍']\n",
    "\n",
    "driver = webdriver.Chrome('/Users/leehyounshu/Documents/2024NLP/project/chromedriver-mac-x64/chromedriver')\n",
    "\n",
    "for keyword in keywords: \n",
    "    base_url = 'https://openapi.naver.com/v1/search/blog.json'\n",
    "    encQuery = urllib.parse.quote(keyword)\n",
    "    n_display = 100\n",
    "    sort = 'sim'\n",
    "    \n",
    "    result_list = []\n",
    "    for start in range(1, 1000, 100):\n",
    "        url = f'{base_url}?query={encQuery}&display={n_display}&start={start}&sort={sort}'\n",
    "\n",
    "        my_request = urllib.request.Request(url)\n",
    "        my_request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        my_request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "\n",
    "        response = urllib.request.urlopen(my_request)\n",
    "\n",
    "        rescode = response.getcode()\n",
    "        if(rescode==200):\n",
    "            response_body = response.read()\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "            \n",
    "        search_results = response_body.decode('utf-8')\n",
    "        search_results = eval(search_results)\n",
    "        result_list.append(search_results)\n",
    "    \n",
    "    blog_links = [] # 블로그 링크를 저장할 리스트 생성\n",
    "    for result in result_list:\n",
    "        for item in (result['items']):\n",
    "            # 링크 추출 후 HTML 엔티티를 디코딩하고 역슬래시를 제거\n",
    "            link = html.unescape(item['link']).replace('\\\\', '')\n",
    "            if 'blog.naver.com' in link: # 네이버 블로그 링크인지 확인\n",
    "                blog_links.append(link) # 네이버 블로그 링크를 리스트에 추가\n",
    "        \n",
    "    all_results = dict() # 모든 결과를 저장할 딕셔너리 생성\n",
    "\n",
    "    for idx in tqdm(range(len(blog_links))):\n",
    "        link = blog_links[idx] # 현재 인덱스의 링크 추출\n",
    "        driver.get(link) # 웹 드라이버로 해당 링크 열기\n",
    "        time.sleep(1) # 페이지 로딩을 위해 1초 대기\n",
    "        \n",
    "        # 웹 드라이버를 해당 iframe으로 전환\n",
    "        iframe = driver.find_element('id', 'mainFrame')\n",
    "        driver.switch_to.frame(iframe)\n",
    "        \n",
    "        try: # 제목 요소 찾기\n",
    "            title = driver.find_element(By.CLASS_NAME, 'pcol1')\n",
    "        except NoSuchElementException: # 요소를 찾을 수 없으면 다음 링크로 넘어감\n",
    "            continue        \n",
    "        title = title.text\n",
    "        \n",
    "        try: # 본문 요소 찾기\n",
    "            body = driver.find_element(By.CLASS_NAME, 'se-main-container')\n",
    "        except NoSuchElementException: # 요소를 찾을 수 없으면 다음 링크로 넘어감\n",
    "            continue \n",
    "        body = body.text.replace('\\n', '') # 본문 텍스트 추출 후 줄바꿈 문자 제거\n",
    "        \n",
    "        all_results[idx] = dict() # 인덱스를 키로 사용하여 딕셔너리 초기화\n",
    "        # 결과 딕셔너리에 URL, 제목, 본문 추가\n",
    "        all_results[idx]['url'] = link\n",
    "        all_results[idx]['title'] = title\n",
    "        all_results[idx]['content'] = body\n",
    "\n",
    "    df = pd.DataFrame(all_results).T\n",
    "    \n",
    "    df.to_csv('csv/{}_블로그.csv'.format(keyword), encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 문서 수:  16312\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임을 저장할 리스트 생성\n",
    "dfs = []\n",
    "\n",
    "# CSV 파일 목록\n",
    "files = ['네이버_뉴스.csv', '카카오_뉴스.csv', '라인_뉴스.csv', '쿠팡_뉴스.csv', '배달의민족_뉴스.csv', '당근마켓_뉴스.csv','토스_뉴스.csv', '직방_뉴스.csv', '야놀자_뉴스.csv', '삼성전자_뉴스.csv', 'SK하이닉스_뉴스.csv', 'DB하이텍_뉴스.csv',\n",
    "         '네이버_블로그.csv', '카카오_블로그.csv', '라인_블로그.csv', '쿠팡_블로그.csv', '배달의민족_블로그.csv', '당근마켓_블로그.csv', '토스_블로그.csv', '직방_블로그.csv', '야놀자_블로그.csv', '삼성전자_블로그.csv', 'SK하이닉스_블로그.csv', 'DB하이텍_블로그.csv']\n",
    "\n",
    "for file in files:\n",
    "    try: # UTF-8 인코딩으로 CSV 파일 읽기\n",
    "        df = pd.read_csv('csv/'+file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try: # UTF-8 인코딩이 실패하면 CP949 인코딩으로 읽기 시도\n",
    "            df = pd.read_csv('csv/'+file, encoding='cp949')\n",
    "        except UnicodeDecodeError: # CP949 인코딩이 실패하면 EUC-KR 인코딩으로 읽기 시도\n",
    "            df = pd.read_csv('csv/'+file, encoding='euc-kr')\n",
    "\n",
    "    keyword = file.split(\".\")[0] # 파일명에서 키워드 추출 (확장자 제외)\n",
    "\n",
    "    df[\"keyword\"] = keyword # 데이터프레임에 'keyword' 열 추가\n",
    "    dfs.append(df) # 데이터프레임 리스트에 추가\n",
    "\n",
    "df_combined = pd.concat(dfs, ignore_index=True) # 모든 데이터프레임을 하나로 결합, 인덱스 무시\n",
    "\n",
    "# 'content' 열에서 중복된 행 제거, 처음 등장하는 행 유지\n",
    "df_combined.drop_duplicates(['content'], keep='first', ignore_index=True, inplace=True)\n",
    "# 'content' 열에서 앞쪽 공백 제거\n",
    "df_combined[\"content\"] = df_combined[\"content\"].str.replace(\"^ +\", \"\")\n",
    "# 'content' 열에서 빈 문자열을 NaN으로 대체\n",
    "df_combined[\"content\"] = df_combined[\"content\"].replace(\"\", np.nan) \n",
    "# NaN이 포함된 행 제거\n",
    "df_combined = df_combined.dropna(how=\"any\")\n",
    "# 인덱스 재설정, 이전 인덱스 삭제\n",
    "df_combined = df_combined.reset_index(drop=True)\n",
    "\n",
    "print('총 문서 수: ', len(df_combined)) # 총 문서 수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>keyword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/366/000...</td>\n",
       "      <td>[전문] 민희진 “네이버·두나무, 투자 무관한 사적 만남”</td>\n",
       "      <td>민 대표 “하이브가 제시한 증거 자료 모두 불법”하이브 “증거 짜깁기한 적 없어”…...</td>\n",
       "      <td>네이버_뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/015/000...</td>\n",
       "      <td>\"네이버·두나무와의 만남은…\" 민희진 첫 입장 표명</td>\n",
       "      <td>민희진 어도어 대표, 기자회견 후 첫 입장\"네이버, 두나무, 하이브와 4자 대면 하...</td>\n",
       "      <td>네이버_뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/011/000...</td>\n",
       "      <td>[전문] 기자회견 후 첫 입장 발표…민희진 “네이버·두나무, 투자 무관한 사적 만남\"</td>\n",
       "      <td>서울경제스타DB[서울경제]민희진 어도어 대표가 경영권 확보를 위해 네이버와 업비트 ...</td>\n",
       "      <td>네이버_뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/079/000...</td>\n",
       "      <td>민희진 \"네이버·두나무 인수 제안 NO…뉴진스와 더 돈독\"</td>\n",
       "      <td>민희진 어도어 대표 19일 공식 입장문 발표\"하이브 제시 증거 모두 불법 취득\"민희...</td>\n",
       "      <td>네이버_뉴스</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://n.news.naver.com/mnews/article/081/000...</td>\n",
       "      <td>민희진 “네이버·두나무 만남은 사적 자리…카톡 내용도 짜집기”</td>\n",
       "      <td>민희진, 기자회견 후 첫 본인명의 입장 표명발언하는 민희진 어도어 대표 - 민희진 ...</td>\n",
       "      <td>네이버_뉴스</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://n.news.naver.com/mnews/article/366/000...   \n",
       "1  https://n.news.naver.com/mnews/article/015/000...   \n",
       "2  https://n.news.naver.com/mnews/article/011/000...   \n",
       "3  https://n.news.naver.com/mnews/article/079/000...   \n",
       "4  https://n.news.naver.com/mnews/article/081/000...   \n",
       "\n",
       "                                             title  \\\n",
       "0                 [전문] 민희진 “네이버·두나무, 투자 무관한 사적 만남”   \n",
       "1                     \"네이버·두나무와의 만남은…\" 민희진 첫 입장 표명   \n",
       "2  [전문] 기자회견 후 첫 입장 발표…민희진 “네이버·두나무, 투자 무관한 사적 만남\"   \n",
       "3                 민희진 \"네이버·두나무 인수 제안 NO…뉴진스와 더 돈독\"   \n",
       "4               민희진 “네이버·두나무 만남은 사적 자리…카톡 내용도 짜집기”   \n",
       "\n",
       "                                             content keyword  \n",
       "0  민 대표 “하이브가 제시한 증거 자료 모두 불법”하이브 “증거 짜깁기한 적 없어”…...  네이버_뉴스  \n",
       "1  민희진 어도어 대표, 기자회견 후 첫 입장\"네이버, 두나무, 하이브와 4자 대면 하...  네이버_뉴스  \n",
       "2  서울경제스타DB[서울경제]민희진 어도어 대표가 경영권 확보를 위해 네이버와 업비트 ...  네이버_뉴스  \n",
       "3  민희진 어도어 대표 19일 공식 입장문 발표\"하이브 제시 증거 모두 불법 취득\"민희...  네이버_뉴스  \n",
       "4  민희진, 기자회견 후 첫 본인명의 입장 표명발언하는 민희진 어도어 대표 - 민희진 ...  네이버_뉴스  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['네이버', '카카오', '라인', '쿠팡', '배달의민족', '당근마켓', '토스', '직방', '야놀자', '삼성전자', 'SK하이닉스', 'DB하이텍']\n",
    "\n",
    "for keyword in keywords:\n",
    "    filtered_df = pd.DataFrame() # 빈 데이터프레임 생성\n",
    "\n",
    "    # 'keyword' 열에서 현재 키워드를 포함하는 행을 필터링\n",
    "    filtered_rows = df_combined[df_combined['keyword'].str.contains(keyword, na=False)]   \n",
    "    # 필터링된 행들을 빈 데이터프레임에 추가\n",
    "    filtered_df = pd.concat([filtered_df, filtered_rows], ignore_index=True)\n",
    "    # 'keyword' 열 삭제\n",
    "    filtered_df.drop(\"keyword\", axis=1, inplace=True)\n",
    "    # 필터링된 데이터를 새로운 CSV 파일로 저장\n",
    "    filtered_df.to_csv('csv_com/{}.csv'.format(keyword), encoding='utf-8-sig', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
